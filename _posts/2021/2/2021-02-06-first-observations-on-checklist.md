---
title: "First observations attempting to use my API documentation checklist"
categories:
- technical-writing
permalink: /blog/observations-using-checklist/
keywords: product overviews
description: "I recently published a comprehensive checklist for evaluating documentation quality. I noted that my perspective is more evolving and experiential, not something tried and true from years of practice. I have a few initial observations since publishing this."
bitlink:
---

## Observation 1: Limit the scope

My first observation is that developer portals are collaborative spaces where a lot of different writers interact, and you might own only a small piece of the portal. For example, there might be docs for 20 different products, with many different contributing teams. You might own docs for 2-3 products only. The user journey, however, might span the entire portal. Despite this, it's too daunting to focus on the entire portal in an initial assessment. It's more realistic to focus on the scope you own, at least initially.

## Observation 2: Two levels for checklists

Many characteristics for docs can only be assessed when you're much more familiar with the docs. For example, you can't evaluate whether the steps in docs are accurate until you can go through them or gather more feedback from users. This isn't feasible without much more familiarity with the docs. In fact, you can't even tell if screenshots or other visuals are accurate without more familiarity. Because of this, I separated the checklist into a [first-level](/learnapidoc/docapis_metrics_first_level_checklist) and [second-level](/learnapidoc/docapis_metrics_second_level_checklist) checklist.

The first-level checklist lists criteria you can evaluate without more familiarity with the content. For example, you can see if there's a getting started tutorial, a product overview, code samples, etc. Then later you can assess whether the instructions are consistent from topic to topic, whether the code in the tutorials matches any sample apps available, whether the terms listed in the glossary match usage in the docs, and so on.

## Observation 3: Checklists aren't scannable

Checklists should be more scannable. I added bold summaries for each checklist item to make it more consumable at a glance.

## Observation 4: Page was too long

My initial page on measuring impact was about 8,000 words. I'm impressed that so many people actually read it from beginning to end. However, I decided to make the content more readable by breaking it up into multiple pages. I moved the content out of the processes section into its own section on [Metrics and measurement](/learnapidoc/docapis_metrics_and_measurement). Now that section is about four pages.

I plan to expand this section with additional info on metrics that go beyond documentation quality (though I don't have definitive plans for topics). I'm still trying to figure out what actually works when measuring API documentation quality. I'm not there yet. My content in this course is on the practical &mdash; the techniques should actually work in practice, not just be theoretical.
